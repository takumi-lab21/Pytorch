{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テンソル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 配列から初期化できる\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "print(type(x_data))\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# テンソルの属性\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x_dataの属性を維持して新しいテンソルを生成\n",
    "x_ones = torch.ones_like(x_data) \n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テンソルの操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPUが使用可能であれば、GPU上にテンソルを移動させる\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1, 2, 3])\n",
      "First column:  tensor([1, 4, 7])\n",
      "Last column: tensor([3, 6, 9])\n",
      "Last row:  tensor([7, 8, 9])\n",
      "tensor([[1, 0, 3],\n",
      "        [4, 0, 6],\n",
      "        [7, 0, 9]])\n"
     ]
    }
   ],
   "source": [
    "# numpy likeな操作\n",
    "tensor = torch.tensor(\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9],\n",
    "    ]\n",
    ")\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "print('Last row: ', tensor[-1, ...])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "\n",
    "\"\"\"MEMO\n",
    "...は、pythonの組み込み定数である、Ellipsisというもの。\n",
    "print(Ellipsis)\n",
    "print(...)\n",
    "...をインデックスに指定した場合、途中の(複数の)次元を省略できる。\n",
    ":を指定した場合の複数形と捉えれば良い。\n",
    "参考: https://note.nkmk.me/python-numpy-ellipsis/\n",
    "\n",
    "\n",
    "\n",
    "-1をインデックスに指定した場合、「最後の要素」を表すインデックスとして利用されている。\n",
    "\n",
    "import numpy as np\n",
    "a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(a[-1 : 100])# -1は最後の一つを表す\n",
    "print(a[-1])\n",
    "\n",
    "\n",
    "参考: https://blog.logicky.com/2017/01/25/python3-numpyとpythonの配列のスライスでマイナス使った場合/\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\"\"\"Torch Visionとは\n",
    "torchvision:\n",
    "    公式Doc: https://pytorch.org/vision/stable/index.html\n",
    "    画像タスクに対するデータセット、モデル、後処理、前処理等のスクリプトがまとまっている。\n",
    "    \n",
    "torchvision.datasets:\n",
    "    多くのデータセットが利用できる\n",
    "    ここにあるだけでもかなりのことができる。\n",
    "\n",
    "mnist: \n",
    "    本家: http://yann.lecun.com/exdb/mnist/\n",
    "    有名なデータセット集\n",
    "\n",
    "LSUN:\n",
    "    本家: https://www.yf.io/p/lsun\n",
    "    室内風景のデータセット\n",
    "    \n",
    "ImageFolder:\n",
    "    フォルダ分けされたデータセットを扱うためのDataset、Dataloaderが実装\n",
    "    されている。\n",
    "\"\"\";\n",
    "\n",
    "\n",
    "\"\"\"DatasetとDataloader\n",
    "Dataset:\n",
    "    データを扱うためのpytorchのクラス。\n",
    "    このクラスを拡張してDatasetクラスを作成する必要がある。\n",
    "必須要件:\n",
    "    __init__\n",
    "    初期化処理\n",
    "    \n",
    "    __getitem__\n",
    "    データセットのdata, labelを返す関数\n",
    "    \n",
    "    __len__:\n",
    "    データセットの長さを返す関数\n",
    "\n",
    "Dataloader:\n",
    "    Datasetからデータを取り出すためのクラス\n",
    "    dataloader.__iter__()により、iterに変換した上で、\n",
    "    for文でループを回せば良い。\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9\n",
      "data.shape: torch.Size([1, 28, 28])\n",
      "label: 9\n",
      "data.shape: torch.Size([1, 28, 28])\n",
      "len(batch): 2\n",
      "label: 9\n",
      "data.shape: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Datsetクラスから直接取り出す\n",
    "## 最初の要素をタプルとして取得\n",
    "data, label = (training_data.__getitem__(0))\n",
    "\n",
    "print(f\"label: {label}\")\n",
    "print(f\"data.shape: {data.shape}\")\n",
    "\n",
    "## 配列のような取得もできる\n",
    "# __getitem__は、Pythonの特殊メゾッドなので、以下のようにできる。\n",
    "data, label = (training_data[0])\n",
    "print(f\"label: {label}\")\n",
    "print(f\"data.shape: {data.shape}\")\n",
    "\n",
    "# dataloaderで中身を取り出す\n",
    "train_loader = DataLoader(training_data, batch_size=10, shuffle=False)\n",
    "\n",
    "for batch in iter(train_loader):\n",
    "    # batchサイズの個数ずつ取り出される。\n",
    "    print(f\"len(batch): {len(batch)}\")\n",
    "    \n",
    "    data, label = batch[0][0], batch[1][0]\n",
    "    print(f\"label: {label}\")\n",
    "    print(f\"data.shape: {data.shape}\") \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "\"\"\"Transform\n",
    "Transforms:\n",
    "    データの変換を扱うクラス\n",
    "    Datasetクラスの引数に渡すと、データを読み込むタイミングで変換を実行することができる。\n",
    "    データを変換する際には、transform, \n",
    "    ラベルを変換するためには、target_transformの引数に設定する。\n",
    "\n",
    "Lambda Transforms\n",
    "    ユーザーが用意した関数を変換に利用できる仕組み。\n",
    "    上の例では、スカラーをワンホット行列に変換する処理を指定している。\n",
    "    y = torch.tensor(8)\n",
    "    res = torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)\n",
    "    print(res)\n",
    "    >>> tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 処理を実行するデバイスを選択\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの実装\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, \n",
    "                      out_features=416),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=416, \n",
    "                      out_features=416),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=416, \n",
    "                      out_features=10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\"\"\"nn.Module\n",
    "nn.Module:\n",
    "    ニューラルネットワークを構築するためのPytorchのクラス。\n",
    "    モデルを構築する場合には、このクラスを継承して中身を実装する。\n",
    "    最低限、コンストラクタとforward関数を実装する必要がある。\n",
    "\n",
    "forward:\n",
    "    推論時の処理をここに実装する。\n",
    "\n",
    "nn.Linear:\n",
    "    全結合層を扱うためのクラス\n",
    "\n",
    "nn.ReLU:\n",
    "    活性化関数として有名な、「正規化線形関数」を扱うためのクラス。\n",
    "    活性化関数は線形変換のあとに、非線形性を加え、ニューラルネットワークの表現力を向上させる役割をします\n",
    "\n",
    "nn.Flatten\n",
    "    高次元のデータを低次元に加工するレイヤー。\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=416, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=416, out_features=416, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=416, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([416, 784]) | Values : tensor([[ 0.0151,  0.0338,  0.0026,  ...,  0.0151, -0.0332, -0.0208],\n",
      "        [-0.0245,  0.0235,  0.0187,  ..., -0.0346, -0.0055, -0.0249]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([416]) | Values : tensor([ 0.0168, -0.0353], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([416, 416]) | Values : tensor([[ 2.0708e-02, -2.4836e-03, -3.1060e-02, -1.5956e-02, -2.7032e-02,\n",
      "         -4.5709e-02, -1.4766e-02, -2.7931e-02, -3.9110e-02,  3.4169e-02,\n",
      "          4.0767e-02, -1.5781e-02,  3.4974e-02, -3.3648e-02, -2.6193e-02,\n",
      "         -3.1034e-02,  1.8640e-02, -4.5998e-02, -1.1963e-02,  1.4345e-02,\n",
      "          4.5039e-02, -2.4435e-03,  4.5087e-02,  4.8946e-02,  2.1576e-02,\n",
      "          1.2134e-02,  3.0343e-03,  3.2917e-02,  3.0207e-02,  1.6604e-02,\n",
      "          7.6905e-05,  1.1826e-02,  2.6523e-02, -1.1810e-02, -1.1142e-02,\n",
      "         -3.5267e-03,  4.8092e-02,  1.1460e-02, -4.4388e-02, -3.6904e-02,\n",
      "         -3.4004e-02, -2.7728e-02, -4.6279e-02,  1.3595e-02,  2.3141e-02,\n",
      "          2.1134e-02,  1.3260e-02,  1.4680e-02,  8.2774e-03, -2.7026e-02,\n",
      "         -7.4627e-03,  2.6372e-02, -2.1689e-02,  1.5211e-02,  4.9676e-03,\n",
      "         -2.1954e-02, -3.2514e-02,  3.4055e-02,  4.0374e-02,  1.8362e-03,\n",
      "          4.5010e-02,  4.2548e-02, -1.9379e-02, -4.4310e-02, -1.0060e-02,\n",
      "         -9.2256e-03,  1.5535e-03,  4.4901e-05, -1.0412e-02, -2.6746e-02,\n",
      "          1.7191e-02,  2.5829e-02,  2.1822e-02,  3.0672e-02,  2.6501e-02,\n",
      "          7.9285e-03,  1.3798e-03,  1.2502e-02,  3.2482e-02, -8.6265e-03,\n",
      "          3.2761e-02,  3.1995e-02,  5.1196e-04, -2.8739e-02, -2.6426e-02,\n",
      "          1.2933e-02,  3.6589e-02,  1.5291e-02, -5.0340e-03,  5.8476e-03,\n",
      "          3.0678e-02, -2.9560e-02, -2.0853e-03, -3.0443e-02, -1.4064e-02,\n",
      "         -9.6953e-03, -1.6509e-02,  5.6046e-03,  7.7866e-03, -3.1715e-02,\n",
      "         -2.6641e-02, -3.8611e-02, -4.4950e-02, -2.2417e-02,  3.6995e-04,\n",
      "          3.5775e-02, -3.2352e-02,  1.7363e-02, -2.6681e-02, -3.5492e-02,\n",
      "         -4.8347e-02, -3.2972e-02, -4.5627e-02, -1.1037e-02, -3.6696e-02,\n",
      "          3.1793e-02, -3.0214e-02,  3.3988e-02,  3.9223e-02, -5.5843e-03,\n",
      "          3.7674e-02, -3.7885e-02, -1.4833e-02, -4.3332e-02, -1.3307e-02,\n",
      "          1.1379e-02,  3.3208e-02, -7.5247e-03, -4.7032e-02, -3.7997e-02,\n",
      "         -1.2540e-02,  1.4422e-02,  2.0749e-02,  2.4592e-02, -1.3342e-03,\n",
      "          1.3915e-02, -7.5968e-03, -4.2665e-02,  4.8373e-02, -9.5396e-03,\n",
      "          4.4472e-02,  4.0943e-02, -2.6406e-02,  2.3585e-02, -4.1311e-02,\n",
      "          1.8166e-02, -3.4549e-02, -7.9228e-03,  4.4641e-02,  1.9040e-02,\n",
      "         -2.9656e-03,  4.6813e-02,  2.4083e-02,  3.5294e-02, -1.9294e-02,\n",
      "         -1.7788e-02,  1.2325e-02,  3.9749e-02, -4.2751e-02, -2.1662e-02,\n",
      "          1.5467e-03, -6.8512e-03,  3.4463e-02, -1.6983e-02, -3.9586e-02,\n",
      "          1.4240e-03,  4.2972e-02,  1.4506e-03,  2.8595e-02, -7.4929e-04,\n",
      "         -3.0811e-02, -2.3594e-02, -6.3764e-03, -4.6714e-02, -4.1254e-02,\n",
      "          2.6777e-02,  1.1444e-02,  4.1494e-02,  4.2818e-02, -2.0399e-02,\n",
      "          4.9341e-03,  3.8022e-03, -3.8676e-02,  4.3357e-02, -4.7522e-02,\n",
      "          3.5933e-02, -1.6387e-02, -1.1960e-02,  5.1583e-03, -4.3615e-03,\n",
      "         -4.8843e-02,  3.7806e-02, -1.3480e-02,  3.9047e-02,  1.0893e-02,\n",
      "          3.5109e-02, -7.7115e-04,  4.0897e-02, -2.6520e-02, -9.4150e-03,\n",
      "          9.2115e-03, -2.7295e-02, -2.5810e-02, -3.4410e-02, -2.1559e-03,\n",
      "         -1.8480e-02, -3.6323e-02,  2.7548e-02,  1.2683e-02, -1.8363e-02,\n",
      "          3.5957e-02,  1.1524e-03,  3.6069e-02, -2.5855e-02,  2.4010e-02,\n",
      "          3.5340e-02, -3.7338e-03,  2.2601e-02,  1.5674e-02,  1.2284e-02,\n",
      "         -4.7938e-03, -1.6936e-02,  5.8376e-03, -1.7036e-02, -9.6869e-04,\n",
      "          4.0101e-02,  3.2569e-02, -4.7189e-02,  1.0044e-03, -2.2006e-02,\n",
      "          3.2992e-02, -3.7041e-02, -1.2670e-02, -7.2445e-03,  3.6149e-02,\n",
      "          1.5236e-02,  3.9632e-02, -1.3630e-02, -7.1366e-03,  4.7149e-02,\n",
      "         -3.7038e-03,  2.3569e-02,  3.1352e-02,  2.4600e-02, -4.5147e-02,\n",
      "          2.8483e-02, -3.9433e-03, -1.0084e-02, -1.8524e-02,  1.6507e-02,\n",
      "          2.8847e-02, -2.8102e-02, -4.3194e-02, -4.6041e-03, -3.7276e-02,\n",
      "         -2.2227e-02,  4.8965e-02,  9.8309e-03,  3.2230e-03, -2.3999e-02,\n",
      "          2.1950e-02,  2.1801e-02,  3.7462e-02,  2.1694e-02, -4.7348e-03,\n",
      "         -1.9735e-02,  3.5256e-02, -4.1092e-02,  2.9073e-02, -3.8192e-02,\n",
      "         -1.7481e-02, -3.0019e-02,  9.6105e-04, -1.5957e-02, -8.0463e-03,\n",
      "         -7.7879e-03,  3.4534e-02,  3.5674e-03, -2.3857e-02,  3.8167e-02,\n",
      "          1.3473e-02,  4.2433e-02, -3.7277e-02,  3.9937e-02,  2.6164e-02,\n",
      "          3.8435e-02,  3.7010e-02,  4.6619e-02, -2.5553e-02,  4.7251e-02,\n",
      "         -2.6705e-02,  3.6340e-02, -2.9483e-02, -3.9688e-02, -3.7483e-02,\n",
      "          2.9401e-02,  3.6086e-02,  2.9250e-02, -1.2933e-02, -4.0695e-02,\n",
      "          1.0666e-02, -2.8755e-02, -2.4818e-02, -1.1876e-02,  3.6490e-03,\n",
      "          4.6410e-02, -4.4604e-02, -6.4262e-03, -2.3187e-02,  3.7665e-02,\n",
      "          5.4039e-03,  3.9557e-02, -1.1978e-02, -6.0515e-04,  9.1081e-03,\n",
      "          3.3772e-02, -3.5913e-02,  3.5822e-02, -4.0684e-02, -7.7041e-03,\n",
      "         -1.9363e-03, -2.0961e-02,  1.5300e-02,  2.9705e-02, -5.0178e-04,\n",
      "         -1.5912e-02,  4.1758e-02, -3.4721e-02,  2.1454e-02, -1.3464e-02,\n",
      "          2.4680e-02,  3.4144e-02, -1.6432e-02, -5.5550e-03,  1.1756e-02,\n",
      "          4.8973e-02,  4.1774e-02,  4.1785e-02,  3.7280e-02, -4.6571e-02,\n",
      "         -3.5789e-02,  2.7153e-02,  2.4091e-03, -4.2589e-02, -4.0977e-02,\n",
      "          6.6029e-03,  1.0478e-02, -1.8467e-02,  9.1024e-03,  2.9289e-02,\n",
      "         -2.8483e-02, -3.9919e-03,  2.5879e-02,  2.0368e-02,  1.2016e-02,\n",
      "         -4.0054e-02, -1.3056e-02, -4.4228e-02,  9.2928e-03,  4.2335e-02,\n",
      "         -2.0454e-02,  4.1047e-02, -3.1110e-02, -4.0541e-02, -1.7460e-02,\n",
      "         -1.7710e-02, -2.3785e-02,  4.3862e-02, -8.7249e-03,  3.7513e-02,\n",
      "          3.1579e-02,  2.6414e-02,  3.3745e-03,  4.0060e-02, -4.4701e-02,\n",
      "         -3.4939e-02, -3.0440e-02,  1.0633e-02,  4.8378e-02, -3.8707e-02,\n",
      "          1.6855e-03,  3.5417e-02,  3.2443e-02,  2.7206e-02, -3.8250e-02,\n",
      "          8.3215e-03,  6.1073e-03,  1.1347e-02,  1.3435e-02, -2.6756e-02,\n",
      "         -4.1132e-02,  1.3944e-02, -7.0227e-03,  2.3513e-02, -1.5368e-02,\n",
      "          2.3768e-03,  4.8127e-02, -3.1945e-02, -2.5948e-02, -2.6898e-02,\n",
      "          2.3597e-02, -3.2336e-02, -1.2466e-02, -4.4558e-02,  4.3466e-02,\n",
      "          4.6357e-02, -3.4313e-02, -1.8031e-02, -2.0753e-02,  4.0111e-02,\n",
      "          4.5201e-02, -1.1709e-02, -3.3560e-02,  3.4591e-02, -3.8992e-02,\n",
      "          4.2822e-03],\n",
      "        [-2.3889e-02,  3.3354e-02, -9.8526e-03,  3.7835e-02, -4.0343e-02,\n",
      "          3.6675e-03,  3.2482e-02, -4.5205e-02, -1.6541e-02, -2.1834e-03,\n",
      "          3.4612e-04,  1.2137e-03, -4.8755e-02, -5.1520e-03,  3.2266e-03,\n",
      "          1.6594e-03, -1.4690e-02,  3.7841e-02,  4.6565e-02, -4.0556e-02,\n",
      "         -2.4356e-02, -3.3883e-02, -1.2406e-02,  1.4875e-03, -4.7121e-02,\n",
      "          2.8660e-02, -2.6252e-02,  3.1133e-02, -3.0145e-02,  3.3117e-02,\n",
      "          1.2744e-02, -9.0966e-03,  6.4470e-03, -8.2959e-03, -5.4222e-03,\n",
      "         -3.3353e-02, -1.4312e-02, -1.6118e-02, -3.9144e-02,  3.5153e-02,\n",
      "         -4.6056e-02,  4.1506e-02,  1.8510e-02,  3.3117e-02, -3.7114e-02,\n",
      "          2.4611e-02,  4.0482e-02, -4.5220e-02, -4.6870e-04,  3.6485e-02,\n",
      "         -2.2782e-02, -3.0567e-02,  2.8088e-02,  1.8393e-02,  2.0567e-02,\n",
      "         -1.8681e-02,  3.9802e-02, -2.6672e-02, -2.4143e-02,  1.9845e-02,\n",
      "         -3.5003e-02,  3.3706e-02, -4.1053e-02, -3.2099e-02,  4.7762e-02,\n",
      "         -2.6010e-02, -1.7426e-02,  4.4746e-03, -2.6593e-02, -1.4454e-02,\n",
      "          4.7402e-02, -3.0192e-02,  4.3297e-03,  2.3223e-02,  4.3711e-02,\n",
      "         -9.0245e-03,  8.0673e-03,  1.9472e-02,  1.8802e-02,  3.8175e-02,\n",
      "         -3.6667e-02, -3.3462e-02,  8.3312e-03,  4.1116e-02,  2.8330e-03,\n",
      "          3.3079e-02,  3.1030e-03, -8.3115e-03,  5.7283e-03,  4.1810e-02,\n",
      "         -4.6386e-02, -3.4984e-02,  1.8054e-02, -4.9437e-03, -2.7782e-02,\n",
      "          8.2918e-03, -3.4759e-02, -1.8444e-03,  4.4566e-02,  1.7217e-02,\n",
      "         -8.6665e-03, -2.9219e-03, -4.7103e-02,  4.2075e-02, -2.5788e-02,\n",
      "         -3.7213e-02,  3.6454e-02, -1.0370e-02,  4.0287e-02, -2.0759e-02,\n",
      "         -1.1412e-02, -1.7598e-02,  9.7719e-03,  2.3492e-02, -1.2982e-02,\n",
      "          1.3825e-02,  4.5927e-02,  1.8715e-02, -3.7251e-02,  4.6959e-02,\n",
      "          4.2249e-02,  4.6331e-02, -2.1306e-02, -2.3814e-02,  4.1829e-02,\n",
      "          3.5448e-02, -1.7120e-02,  1.4382e-02, -2.3540e-02,  2.9105e-02,\n",
      "          3.6804e-02,  1.7479e-02,  2.4788e-02,  1.6509e-03,  4.5656e-02,\n",
      "          5.4520e-03,  2.3532e-02, -9.3438e-04, -3.8624e-02, -4.2446e-02,\n",
      "         -1.5313e-02, -3.8853e-02, -1.2570e-02,  2.9031e-03, -2.0893e-02,\n",
      "          1.6184e-02,  4.4544e-02, -2.4923e-02,  3.5922e-02,  4.6159e-02,\n",
      "          2.4478e-02,  6.3559e-03,  2.5231e-02, -2.1592e-02,  4.3688e-02,\n",
      "          2.1980e-02, -3.2167e-02,  2.5482e-02, -1.6430e-02, -8.8979e-04,\n",
      "         -4.4449e-02, -3.5518e-02, -4.8362e-02,  3.6447e-02, -6.1125e-03,\n",
      "         -2.7501e-02, -2.6499e-02, -4.1732e-03, -4.2459e-02,  2.9429e-02,\n",
      "          1.4300e-03,  1.9949e-02, -4.7329e-03,  2.1552e-02,  1.3388e-02,\n",
      "         -1.0097e-02,  1.1939e-02,  7.4013e-04, -3.8387e-02, -9.0282e-03,\n",
      "          2.0785e-02, -8.3811e-03, -1.5837e-02, -1.8891e-02, -4.8718e-02,\n",
      "         -1.3249e-02, -4.2912e-02, -3.2193e-02,  2.5910e-02, -3.1591e-02,\n",
      "          4.1940e-02,  1.0053e-02, -2.1828e-02, -4.0669e-02, -3.8322e-02,\n",
      "         -2.6318e-02, -3.7932e-02, -4.5839e-02,  1.5283e-02, -9.9711e-03,\n",
      "          6.8590e-03, -7.0370e-04,  1.6188e-03, -2.0090e-02, -1.2534e-02,\n",
      "         -3.5053e-03, -4.2266e-02, -3.7420e-03, -2.9823e-02, -1.5980e-02,\n",
      "         -3.9314e-02,  3.6601e-02,  1.0979e-02,  2.2362e-03, -2.9015e-02,\n",
      "         -5.7166e-03,  3.9123e-02,  3.1836e-02,  3.8607e-02,  1.6985e-02,\n",
      "         -3.3595e-02, -1.7209e-02, -2.4958e-02,  3.3729e-02,  3.0323e-02,\n",
      "          3.0734e-02, -2.7458e-02,  4.8484e-02,  2.9324e-02, -2.7693e-02,\n",
      "         -1.4319e-02,  2.9792e-02, -4.0341e-03, -4.5734e-02,  1.7488e-02,\n",
      "          4.4964e-02,  4.1137e-02, -3.8114e-02, -2.9529e-02,  1.1377e-02,\n",
      "          4.3779e-02,  4.7066e-02, -3.2072e-02, -4.2303e-02,  3.5146e-02,\n",
      "         -2.7726e-02, -2.2583e-02,  3.0015e-02, -3.9711e-02, -1.2765e-02,\n",
      "          3.3136e-02, -6.5540e-03,  1.7476e-03, -2.5615e-02,  8.2972e-03,\n",
      "          2.1748e-03, -2.9519e-02, -1.6712e-02,  1.2162e-02, -2.7483e-03,\n",
      "         -3.9193e-02,  4.0373e-02, -1.8624e-02,  4.2543e-02,  3.6143e-02,\n",
      "         -1.1845e-02, -1.9202e-02, -6.6804e-04,  4.8763e-02,  3.3598e-02,\n",
      "          3.1415e-02,  3.7295e-02, -3.2492e-02,  3.0846e-02, -3.4374e-02,\n",
      "         -1.0390e-02, -4.0850e-02,  2.7498e-02, -1.4595e-02, -2.4507e-02,\n",
      "         -9.7206e-03, -2.1689e-02, -1.7638e-02, -2.6592e-02,  1.1133e-02,\n",
      "         -2.6944e-02,  8.0812e-03, -3.0368e-02, -1.9308e-02,  4.7845e-03,\n",
      "          1.6269e-02,  3.1816e-02,  4.7189e-02, -2.1388e-02,  6.4847e-03,\n",
      "         -1.0645e-02, -1.0433e-02,  6.8662e-03,  3.0307e-02, -2.2347e-02,\n",
      "         -4.2730e-02,  1.6624e-02,  1.0218e-02,  4.6916e-04,  1.3405e-02,\n",
      "         -2.6180e-02, -4.5694e-02,  3.4524e-02,  1.8681e-02,  3.3455e-02,\n",
      "          4.4000e-02,  2.5637e-02,  3.7298e-02,  2.2075e-02, -3.0255e-02,\n",
      "          2.0657e-02, -2.6207e-02, -3.9515e-02, -1.8393e-02,  1.6840e-02,\n",
      "         -3.6060e-02, -3.4572e-02,  2.6005e-02,  1.9011e-02, -2.0494e-02,\n",
      "          1.3039e-02,  3.1172e-02,  1.4026e-02, -1.5887e-02, -4.4161e-02,\n",
      "          4.8972e-02, -4.2007e-02, -2.3831e-02, -6.6850e-03,  2.3016e-02,\n",
      "          4.8788e-02, -1.3967e-03,  1.3422e-02,  4.0554e-02,  4.6228e-02,\n",
      "          7.8724e-03, -6.3691e-03, -1.9228e-02,  1.6305e-02,  3.0366e-03,\n",
      "         -3.6053e-02, -4.6873e-02,  1.4782e-02, -1.2448e-02,  4.3499e-02,\n",
      "          4.7146e-02, -3.6888e-02, -3.7407e-02,  4.8706e-02,  3.2305e-02,\n",
      "         -2.5619e-02, -2.4770e-02, -1.1977e-02, -4.6686e-02,  8.2372e-03,\n",
      "         -9.2499e-03,  3.7002e-03,  1.2598e-02,  2.0749e-02, -3.0248e-02,\n",
      "          3.1069e-02, -2.3617e-02,  3.8164e-02, -4.8934e-02, -4.7234e-02,\n",
      "         -4.4341e-02, -4.2598e-02, -2.2575e-03, -6.7199e-03,  4.6091e-02,\n",
      "          2.4805e-02,  1.1851e-02, -5.9004e-03, -4.2124e-02, -4.1675e-02,\n",
      "          4.8335e-03, -2.9689e-02, -4.2865e-02, -5.3753e-04,  3.7794e-02,\n",
      "          1.6436e-02, -1.0084e-03,  3.1575e-02, -4.2401e-02, -2.4347e-02,\n",
      "          5.5112e-03,  4.0051e-02,  1.1828e-02, -1.3490e-02,  4.8684e-02,\n",
      "         -2.3372e-04,  1.6682e-02,  3.2084e-02, -2.3711e-02,  3.6142e-02,\n",
      "         -3.1052e-02,  2.4559e-02, -4.7725e-02, -8.5331e-03, -3.1686e-02,\n",
      "         -2.7388e-02, -1.9124e-02,  1.5882e-02,  1.6669e-02, -2.9537e-02,\n",
      "         -2.8815e-02, -1.6170e-02,  4.3369e-02,  2.8663e-02, -2.8771e-02,\n",
      "         -4.4437e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([416]) | Values : tensor([-0.0079,  0.0213], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 416]) | Values : tensor([[ 4.7213e-02,  1.8319e-02, -4.7896e-02, -3.7768e-02,  4.1606e-02,\n",
      "          7.7255e-03,  1.7258e-02, -3.2942e-02, -6.6650e-03,  3.4908e-02,\n",
      "         -3.2667e-02,  4.5703e-03, -1.1941e-02,  1.3141e-02, -4.0643e-02,\n",
      "          2.5559e-02,  2.6164e-02, -1.2086e-02, -1.7971e-02, -3.8407e-02,\n",
      "         -2.1335e-02,  4.6328e-02, -1.4970e-02, -1.2225e-02, -1.2206e-02,\n",
      "         -3.0870e-02, -1.7768e-02,  4.0583e-02, -2.5404e-02,  2.1111e-02,\n",
      "         -2.2626e-02, -4.5903e-02, -3.1000e-02,  3.7909e-02, -7.1608e-03,\n",
      "          2.1401e-02, -4.6237e-02, -1.0030e-02, -1.2234e-02,  4.4840e-02,\n",
      "          3.5263e-03,  2.9595e-02,  4.4557e-02, -1.3572e-03,  1.9022e-02,\n",
      "         -3.6336e-02, -4.7203e-02,  4.0137e-02,  2.7080e-03,  2.3043e-02,\n",
      "         -4.3617e-02,  1.9614e-02, -2.6802e-02, -9.5977e-03, -4.0949e-02,\n",
      "         -2.2325e-02,  3.6922e-02,  1.5918e-02,  2.4806e-02, -1.3996e-02,\n",
      "          2.5651e-02,  1.4299e-02, -1.9435e-02,  1.5044e-02,  3.0337e-02,\n",
      "         -1.2429e-02, -3.5680e-02,  4.7727e-02,  1.1005e-02, -4.3562e-02,\n",
      "          1.4386e-02, -3.1445e-02,  9.8387e-03,  1.1715e-02,  4.5280e-02,\n",
      "          3.9294e-03,  1.3519e-02, -4.6462e-02, -3.3023e-02,  6.0475e-04,\n",
      "         -3.6130e-02,  2.1720e-02, -1.5172e-02,  3.9675e-02,  1.8466e-02,\n",
      "          1.5237e-02,  5.7899e-03,  4.8359e-02, -4.6569e-02,  2.4301e-02,\n",
      "         -2.8078e-02, -2.1958e-02,  3.8867e-02,  4.8979e-03, -3.9075e-02,\n",
      "          1.2357e-02, -1.9971e-02, -2.4646e-02, -6.8348e-03, -3.9395e-03,\n",
      "          4.7269e-02,  1.6297e-02, -4.5382e-02,  3.4671e-02,  4.3182e-02,\n",
      "          1.2402e-02,  2.6736e-02,  8.1088e-03,  4.4159e-02, -1.4150e-03,\n",
      "          1.0984e-02, -4.3555e-02,  3.9128e-02,  1.4705e-02, -2.0943e-02,\n",
      "         -2.7521e-02, -2.1371e-02, -4.9029e-02,  1.4295e-02, -4.3736e-03,\n",
      "         -3.4065e-03,  3.3175e-02,  2.8744e-02,  2.5603e-02, -2.3487e-03,\n",
      "          2.0942e-02,  4.4376e-02, -2.8728e-03,  3.7125e-02, -1.0945e-02,\n",
      "         -4.6263e-02,  4.7570e-02,  2.3542e-04,  5.8486e-03, -1.2471e-02,\n",
      "          7.2631e-04, -4.4737e-02, -4.6118e-02,  3.6584e-02,  4.0234e-02,\n",
      "          2.0787e-02,  1.5392e-02, -1.0589e-02, -2.7006e-02, -4.7545e-02,\n",
      "          9.1577e-03,  3.1880e-02, -1.1164e-03, -1.7724e-02,  3.8512e-02,\n",
      "          4.6428e-02, -4.3912e-03,  9.8076e-04,  8.7454e-03, -1.4734e-02,\n",
      "          4.8819e-02, -4.3917e-03, -3.2370e-03,  5.9448e-03,  2.6779e-02,\n",
      "         -2.5174e-02,  1.3621e-02,  2.4828e-02,  1.2273e-02,  3.4282e-02,\n",
      "          4.8682e-02,  3.5463e-02, -1.6484e-02, -1.5130e-02, -3.2712e-02,\n",
      "         -3.4330e-02,  2.3821e-02, -8.3229e-03, -1.4167e-02,  4.5867e-02,\n",
      "          2.8926e-02,  1.3979e-02, -3.7998e-02, -6.7604e-03,  1.3800e-02,\n",
      "         -1.7058e-02, -1.8548e-02,  2.8070e-02, -2.4014e-02, -2.0741e-02,\n",
      "          3.0993e-02, -2.5365e-03,  3.4828e-02,  2.5842e-02, -2.3233e-02,\n",
      "         -4.4980e-02, -3.5963e-02,  3.3097e-02,  1.8883e-02,  5.6319e-03,\n",
      "          1.7075e-02,  1.3425e-02, -3.0385e-02, -3.4519e-02,  4.4870e-02,\n",
      "          1.3353e-02, -3.6860e-02, -1.1742e-02, -2.4939e-02,  3.1181e-02,\n",
      "          4.6645e-03, -4.7745e-02, -1.3931e-02, -3.2661e-02, -4.5953e-02,\n",
      "          3.0790e-05, -6.8933e-03, -3.8260e-02,  2.6159e-02,  7.7949e-03,\n",
      "          2.8921e-02, -3.9205e-02, -2.0962e-03, -2.0174e-02,  1.9416e-02,\n",
      "          3.9792e-03, -4.0053e-02, -2.6023e-02, -2.5645e-02, -1.2049e-03,\n",
      "         -9.9647e-03,  2.1974e-02, -2.6890e-02, -3.3503e-02,  3.9874e-02,\n",
      "         -1.1753e-02,  3.7818e-02, -4.0306e-02,  2.8020e-02,  3.3491e-02,\n",
      "          2.8472e-02,  4.7668e-02,  3.3163e-02, -1.6636e-02,  2.3786e-02,\n",
      "         -3.3889e-02,  3.3982e-03, -4.4364e-02, -3.8925e-02,  2.6161e-02,\n",
      "          1.0017e-02, -1.7053e-02, -1.5167e-02, -1.1548e-02, -1.5816e-02,\n",
      "          3.9003e-02,  4.0525e-02,  1.1650e-02,  4.4222e-02,  3.2841e-02,\n",
      "          2.0383e-02,  2.1267e-02,  3.9774e-02, -4.5751e-02, -1.1903e-02,\n",
      "          3.4105e-02,  1.5725e-02, -2.8179e-02,  8.1169e-04, -3.5551e-02,\n",
      "         -3.4879e-02, -9.4075e-05, -2.5184e-04,  5.9012e-03,  2.4892e-03,\n",
      "          5.3748e-03,  3.5526e-02, -4.0881e-02,  3.8064e-02, -2.6907e-02,\n",
      "          1.4879e-03,  3.4689e-02, -1.7881e-02,  3.9550e-02,  2.5536e-02,\n",
      "          1.4732e-02,  1.6521e-02,  1.9358e-02,  3.2579e-02,  1.2788e-02,\n",
      "          1.0770e-02, -4.2310e-02, -3.6014e-02, -4.8786e-02, -4.8480e-02,\n",
      "         -3.3675e-02,  2.8980e-02,  3.3403e-02,  3.3801e-02,  3.7657e-03,\n",
      "         -3.9849e-02, -3.6055e-02,  9.5276e-03,  2.8761e-02,  3.3265e-02,\n",
      "          2.6645e-02, -2.1186e-02, -3.5533e-02, -1.1577e-02,  1.2464e-02,\n",
      "          1.3200e-02,  6.1466e-03, -3.5829e-02, -8.0710e-04,  4.3362e-02,\n",
      "         -2.3068e-02, -9.2781e-03, -3.6528e-02,  2.3173e-02, -3.9160e-02,\n",
      "          1.4755e-02, -1.2257e-02, -2.6685e-02,  4.1739e-02,  4.7766e-02,\n",
      "          3.2124e-02,  8.9590e-03,  3.7564e-02,  4.6841e-02,  2.9635e-02,\n",
      "         -1.0768e-02, -2.8494e-02, -1.2105e-02, -1.5522e-02,  1.4110e-02,\n",
      "         -7.0453e-03, -1.7629e-02,  6.4819e-03,  1.3798e-02, -2.1096e-02,\n",
      "         -4.6328e-02, -4.2948e-02,  9.6874e-04,  1.6648e-02,  3.4047e-02,\n",
      "          3.8200e-02, -7.9585e-03,  3.0276e-02,  3.6586e-02,  7.1808e-03,\n",
      "         -4.5670e-02,  3.9824e-02,  2.3605e-02, -4.0352e-02,  1.6848e-02,\n",
      "          7.6562e-03, -2.5725e-02, -4.5302e-02,  1.6926e-02, -2.1644e-02,\n",
      "          3.1262e-02,  2.0498e-02, -3.8992e-02, -1.3448e-03,  9.7057e-04,\n",
      "          4.1864e-02, -3.6213e-02,  2.3569e-02, -4.0647e-02,  9.9754e-04,\n",
      "          4.7369e-02,  1.3211e-02, -2.7540e-02,  4.3040e-02, -2.5207e-02,\n",
      "         -1.0016e-02, -1.4457e-02, -1.1694e-02,  4.3510e-02, -2.0336e-02,\n",
      "         -1.0159e-02, -1.8541e-02,  4.7431e-02,  2.1300e-02,  1.1215e-03,\n",
      "         -1.6282e-02, -3.9421e-02, -3.9633e-02, -3.9741e-02,  4.2718e-02,\n",
      "         -2.8463e-02, -4.1878e-02, -3.9745e-02, -2.9834e-02,  2.5014e-02,\n",
      "          1.4140e-02, -3.7193e-02,  2.3021e-02, -2.3745e-02,  1.9537e-02,\n",
      "          3.4355e-02, -4.5112e-02,  3.0322e-02,  3.1626e-03, -2.3129e-02,\n",
      "         -3.4066e-02,  2.1215e-02,  2.5365e-02,  1.9972e-02,  4.2555e-03,\n",
      "          1.1413e-02,  3.1586e-02,  2.5547e-02,  9.5567e-03,  1.5009e-02,\n",
      "         -4.8986e-02,  1.4857e-02, -2.3742e-02, -4.2278e-02,  3.2533e-02,\n",
      "         -9.1242e-03],\n",
      "        [-9.2086e-03,  4.1282e-04, -1.4374e-02,  3.1156e-02, -2.4825e-02,\n",
      "          3.1981e-02,  4.6185e-02,  4.7237e-02, -2.5241e-02,  1.0010e-02,\n",
      "         -2.6947e-03, -1.1028e-02, -1.6247e-03, -1.6652e-02,  2.6886e-02,\n",
      "          1.9558e-02,  5.1834e-03, -1.0878e-02,  4.2076e-02,  4.8759e-02,\n",
      "          1.7916e-02,  1.4710e-02, -2.3257e-02, -9.4852e-03, -2.6043e-03,\n",
      "          2.2474e-02, -1.5801e-02, -3.0890e-02,  2.7631e-02,  3.1777e-03,\n",
      "         -8.0080e-03,  4.2853e-02, -4.3146e-02, -2.9750e-03,  1.6913e-02,\n",
      "          3.4558e-02, -1.8534e-02,  3.2269e-02, -3.4774e-02,  4.7721e-02,\n",
      "         -4.7135e-02,  1.0697e-02,  2.8058e-02,  2.2853e-02,  1.8428e-02,\n",
      "          6.3322e-03, -3.3284e-02, -4.7743e-02,  4.8983e-02,  2.4673e-02,\n",
      "         -2.8714e-02,  4.1461e-03, -2.6227e-02,  4.4442e-03, -3.5867e-02,\n",
      "         -2.6163e-02,  2.9453e-02,  4.3981e-03, -2.2044e-02, -2.6084e-02,\n",
      "          3.9820e-02,  3.2668e-02,  6.4371e-03, -3.3842e-02, -2.2162e-02,\n",
      "          2.1925e-02, -2.0895e-02, -1.8737e-02, -5.5361e-03, -4.6672e-02,\n",
      "         -7.6129e-03, -6.5103e-03, -3.9907e-02,  2.1021e-02,  2.0094e-02,\n",
      "         -4.0967e-02,  1.8521e-02,  4.2561e-02,  3.2000e-02,  9.7839e-03,\n",
      "          3.9718e-02,  9.9998e-03, -8.2609e-03,  4.6782e-02,  3.3622e-02,\n",
      "         -2.7524e-02, -2.9147e-02, -4.4547e-02, -2.6824e-02, -2.9618e-02,\n",
      "          3.1862e-02,  3.7921e-02, -2.7126e-02,  9.9475e-03,  2.9294e-02,\n",
      "         -3.7477e-02, -7.7827e-03, -3.0933e-02,  2.2387e-02, -4.1998e-02,\n",
      "          4.4707e-02, -3.9665e-02, -1.7933e-02,  3.6001e-02, -7.3649e-03,\n",
      "         -3.7155e-03, -3.5533e-02,  3.0699e-02, -8.2020e-03, -2.4726e-02,\n",
      "         -7.9966e-03,  4.4608e-03, -1.6159e-02, -3.0530e-02,  4.7079e-02,\n",
      "         -3.9111e-02, -1.3017e-02,  1.9190e-03,  7.5831e-03, -1.2086e-02,\n",
      "         -2.9612e-02,  2.0467e-02,  6.9399e-03,  2.0682e-02,  3.2112e-04,\n",
      "          8.4104e-03, -3.8882e-02,  2.9799e-02, -1.4922e-02, -1.5000e-02,\n",
      "         -2.9596e-02, -3.8894e-02,  4.2837e-02,  2.3026e-02,  2.4046e-02,\n",
      "         -2.8755e-03, -1.4209e-02,  5.9118e-03, -3.9786e-02, -3.7919e-03,\n",
      "          4.2919e-02,  1.7627e-02, -2.3361e-02, -2.6426e-02,  3.5995e-02,\n",
      "         -8.6386e-03,  1.2077e-02,  1.8121e-02,  3.0597e-02, -4.7809e-02,\n",
      "          2.2103e-02,  3.0439e-02, -1.7387e-02, -4.2917e-02, -4.2690e-02,\n",
      "         -3.5718e-02, -5.6089e-03,  3.7656e-04,  3.4968e-03, -1.9036e-02,\n",
      "          2.1661e-02, -1.3905e-02,  4.5255e-02,  1.0943e-02, -4.8109e-02,\n",
      "          3.8345e-02,  1.9505e-02, -3.1269e-02,  1.3967e-02, -3.1134e-02,\n",
      "          3.6373e-02,  4.8987e-02,  2.6602e-02, -2.9042e-02, -2.3113e-02,\n",
      "          4.1539e-02, -1.0409e-02, -9.8751e-03, -1.3813e-02,  3.3592e-02,\n",
      "         -4.5875e-02, -4.6438e-02, -3.7677e-02,  1.8190e-02,  4.4438e-02,\n",
      "         -7.3318e-03, -1.3696e-02,  4.7680e-02,  6.7563e-03,  6.9344e-03,\n",
      "         -3.3111e-02,  2.8640e-02,  6.0208e-03,  2.3529e-02, -4.6711e-02,\n",
      "         -8.4853e-03,  3.4386e-02, -3.9909e-02, -2.1162e-02,  2.1867e-03,\n",
      "          2.6470e-02, -5.1885e-03, -4.3617e-02, -8.2113e-03, -4.1111e-02,\n",
      "          3.1200e-02,  4.8210e-02, -4.2539e-05, -4.7785e-02,  4.3387e-02,\n",
      "         -2.7526e-02,  4.2482e-02,  2.8799e-02, -2.7869e-02,  6.1254e-03,\n",
      "         -3.4832e-02, -4.1633e-02,  3.5317e-02, -2.0461e-02,  4.0584e-02,\n",
      "         -4.1260e-02,  3.5318e-02,  3.0750e-02,  2.6112e-02,  3.0795e-02,\n",
      "         -8.7642e-03, -4.1780e-02, -2.4467e-02, -1.5717e-02, -5.8624e-03,\n",
      "         -2.4569e-02, -1.5912e-02,  9.8156e-03,  3.5571e-02, -4.4027e-03,\n",
      "         -2.1247e-02,  3.1116e-02, -4.0965e-02, -4.6564e-02, -2.8444e-02,\n",
      "          2.3106e-02,  4.0006e-02, -8.1964e-03, -2.7260e-02, -1.9277e-02,\n",
      "         -2.1654e-02,  2.4704e-02, -4.8198e-02, -2.2635e-02, -8.4409e-03,\n",
      "          2.3520e-02,  2.1401e-02,  1.3415e-02,  4.1683e-02, -2.5149e-02,\n",
      "         -2.8510e-02, -2.2863e-02,  1.5706e-02,  2.0685e-02,  4.2857e-02,\n",
      "         -3.0484e-02,  2.0664e-02,  2.9993e-02, -4.6826e-02,  1.2238e-02,\n",
      "          3.9709e-02, -3.1355e-03, -6.3661e-03, -1.5464e-02,  3.5045e-02,\n",
      "          2.3995e-02,  3.8334e-02,  4.0607e-02, -7.5386e-03,  4.5390e-02,\n",
      "          2.2270e-02, -1.1127e-02,  1.4312e-02,  4.0821e-02,  2.7553e-02,\n",
      "         -8.3756e-03, -2.8224e-02,  2.7500e-02, -6.3115e-03,  4.1336e-03,\n",
      "         -4.8512e-03, -4.6028e-02, -2.2977e-02,  2.8937e-02,  4.3514e-02,\n",
      "         -3.0696e-02, -1.6006e-02, -2.4620e-02,  1.6768e-02,  2.5825e-02,\n",
      "         -2.7682e-03, -3.7896e-02, -4.4535e-02, -5.6626e-03,  1.8258e-02,\n",
      "         -3.9374e-02,  3.5405e-02,  3.2346e-02, -4.7473e-02,  1.7041e-02,\n",
      "         -1.8155e-02,  4.4723e-02,  3.1333e-02, -1.9088e-02, -2.8879e-02,\n",
      "         -9.8734e-04,  1.7051e-02, -7.0364e-03,  4.3897e-02, -1.3077e-02,\n",
      "          3.9804e-02, -1.5412e-02, -2.8987e-02, -1.8605e-02,  1.3867e-03,\n",
      "          2.0933e-02,  9.1727e-03,  1.6095e-02, -2.9588e-02, -7.4208e-03,\n",
      "          8.8707e-03, -3.0826e-03,  3.6434e-02, -9.4517e-03, -7.4026e-03,\n",
      "          3.3459e-02, -5.0424e-03,  9.2328e-05,  3.3686e-03,  4.5922e-02,\n",
      "          3.9566e-02,  2.7542e-02,  4.0152e-02,  7.7122e-03, -4.7525e-02,\n",
      "         -2.6966e-02, -2.1745e-02, -1.4368e-02,  4.6996e-02,  5.1706e-03,\n",
      "          2.0742e-02, -3.2762e-02, -6.1971e-03, -9.0964e-03, -5.8079e-03,\n",
      "         -4.0891e-02,  1.9374e-02, -8.5635e-03,  4.5385e-02,  2.1630e-02,\n",
      "         -4.1824e-03, -5.9777e-03, -1.6181e-02,  3.3180e-02,  3.0842e-02,\n",
      "         -2.8598e-02,  3.8982e-02, -1.3457e-02, -4.3966e-02, -4.6795e-02,\n",
      "          4.3060e-03,  4.2720e-02,  2.9287e-02,  3.6703e-02, -3.4508e-03,\n",
      "         -2.9214e-03,  2.9365e-02,  6.1193e-03,  2.9837e-02,  4.0800e-02,\n",
      "          4.5105e-02, -1.9992e-02,  1.3451e-02, -4.1375e-02, -1.2327e-03,\n",
      "          1.5214e-02, -3.2092e-02, -3.3310e-03, -2.8531e-02, -4.7140e-02,\n",
      "         -2.8148e-03,  1.8015e-02,  3.5419e-02, -2.3021e-02, -1.6391e-02,\n",
      "         -2.5965e-02, -2.1306e-02,  3.3435e-02, -8.5525e-03, -4.3680e-02,\n",
      "         -4.1222e-02, -4.4110e-02, -3.6452e-02, -1.1785e-02,  1.9462e-02,\n",
      "          6.4500e-03,  2.6429e-02, -2.4716e-02, -4.5874e-03, -5.4323e-03,\n",
      "         -1.3795e-02,  2.5674e-02,  4.5152e-02,  4.4937e-02,  6.0789e-03,\n",
      "          1.4793e-02,  4.3094e-02,  2.0604e-02, -3.7873e-02,  1.9829e-02,\n",
      "          3.5095e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0214, 0.0088], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# モデルの構造を確認\n",
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "# モデルのパラメータを確認\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "\"\"\"\n",
    "NeuralNetwork(\n",
    "(flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "(linear_relu_stack): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
    "    (5): ReLU()\n",
    ")\n",
    ")\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([8])\n"
     ]
    }
   ],
   "source": [
    "# 推論の実行\n",
    "model = NeuralNetwork().to(device)\n",
    "# 乱数で入力画像を作成\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X) \n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([84, 28])\n"
     ]
    }
   ],
   "source": [
    "# Flatten\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten(\n",
    "    start_dim=0,\n",
    "    end_dim=1,\n",
    ")\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "\"\"\"nn.Flatten\n",
    "nn.Flatten\n",
    "高次元のデータを一次元のデータに変換する。\n",
    "start_dimからend_dimまでの次元のデータを1次元のデータに加工する。\n",
    "ex): start_dim=0, end_dim=2,\n",
    "     torch.Size([3, 28, 28])\n",
    "     ↓\n",
    "     torch.Size([2352])\n",
    "\n",
    "ex): start_dim=1, end_dim=2,\n",
    "     torch.Size([3, 28, 28])\n",
    "     ↓\n",
    "     torch.Size([3, 784])\n",
    "\n",
    "ex): start_dim=0, end_dim=1,\n",
    "     torch.Size([3, 28, 28])\n",
    "     ↓\n",
    "     torch.Size([84, 28])\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "torch.Size([2, 9])\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Flattenをより詳細に確かめる\n",
    "input_image = torch.tensor(\n",
    "    [\n",
    "        [[0, 1, 2],\n",
    "        [3, 4, 5],\n",
    "        [6, 7, 8],],\n",
    "        [[0, 1, 2],\n",
    "        [3, 4, 5],\n",
    "        [6, 7, 8],]\n",
    "    ]\n",
    ")\n",
    "flatten = nn.Flatten(\n",
    "    start_dim=1,\n",
    "    end_dim=2,\n",
    ")\n",
    "flat_image = flatten(input_image)\n",
    "print(input_image.size())\n",
    "print(flat_image.size())\n",
    "print(flat_image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db5637f38f20b5db2356bba6e081997c01b7d3b3e14e6b1cb09528d6c22be6b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
